{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a91f8c",
   "metadata": {},
   "source": [
    "#### Trains our transformer model on the tokenized data\n",
    "\n",
    "1. Loads our token sequences and vocabulary \n",
    "2. Build Transformer model (probably going to use Pytorch but we will look into what)\n",
    "3. Train on tokens [:-1] -> [tokens [1:]]\n",
    "4. Save checkpoints and training logs\n",
    "\n",
    "\n",
    "P.S This is subject to lots of change on how we prepare the model I think we will probably be training on multiple datasets and then freezing at different points\n",
    "i.e train first on the accompaniment dataset so it understands general structure and how accompaniment works, then train on dataset with melodies as well so it gets how \n",
    "to play and be reactive based on the melody (we also need to add in a lot of noise and personal data for this part since we will not be perfect soloist like in the jazz\n",
    "songs and MIDI from a Database)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65e5213",
   "metadata": {},
   "source": [
    "Load the token sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fbe845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6eb45ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path.cwd().parent\n",
    "remi_segments_path = project_root / \"remi_segments.jsonl\"\n",
    "vocab_path = project_root / \"vocab.json\"\n",
    "\n",
    "# Load vocab\n",
    "with open(vocab_path, \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# I should use stoi and itos from the vocab\n",
    "stoi = vocab # string to index\n",
    "itos = {int(index): token for token, index in vocab.items()} # index to string\n",
    "\n",
    "\n",
    "# Load token sequences\n",
    "token_sequences = []\n",
    "with open(remi_segments_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        token_sequences.append(record[\"tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5400224a",
   "metadata": {},
   "source": [
    "Encode the tokens as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a76ccce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1640, 1072, 148, 2, 1640, 683, 770, 2, 1640, 235, 490, 2, 1640, 545, 996, 1384, 1308, 1326, 1668, 1301, 1460, 1308, 1326, 1758, 1297, 1564, 1308, 1326, 1758, 1301, 1580, 1308, 1326, 1758, 1301, 1599, 1310, 1349, 1721, 1260, 1310, 1358, 1718, 1260, 1310, 1353, 1718, 1260, 1307, 1318, 1739, 1260, 1309, 1341, 1723, 1263, 1309, 1346, 1727, 1263, 1309, 1349, 1726, 1262, 1308, 1340, 1742, 1250, 1617, 1308, 1333, 1732, 1250, 1633, 1307, 1333, 1737, 1260, 1308, 1340, 1735, 1250, 1634, 1310, 1343, 1716, 1260, 1310, 1349, 1709, 1260, 1310, 1352, 1711, 1260, 1391, 1309, 1343, 1721, 1281, 1309, 1349, 1726, 1281, 1309, 1352, 1730, 1290, 1397, 1308, 1333, 1737, 1250, 1408, 1308, 1340, 1707, 1250, 1414, 1310, 1349, 1713, 1263, 1310, 1354, 1718, 1263, 1310, 1357, 1716, 1263, 1307, 1326, 1738, 1262, 1309, 1349, 1717, 1262, 1309, 1345, 1716, 1263, 1309, 1354, 1721, 1263, 1308, 1340, 1709, 1250, 1431, 1308, 1340, 1723, 1250, 1432, 1308, 1333, 1737, 1250, 1450, 1310, 1351, 1718, 1262, 1310, 1357, 1716, 1262, 1310, 1348, 1713, 1262, 1307, 1329, 1737, 1263, 1308, 1340, 1707, 1250, 1461, 1309, 1345, 1726, 1281, 1309, 1339, 1726, 1281, 1462, 1309, 1348, 1727, 1271, 1308, 1340, 1724, 1250, 1467, 1308, 1333, 1737, 1250, 1478, 1308, 1320, 1731, 1250, 1484, 1309, 1345, 1718, 1296, 1485, 1307, 1334, 1735, 1262, 1309, 1350, 1721, 1296, 1308, 1340, 1733, 1250, 1486, 1309, 1353, 1722, 1290, 1503, 1308, 1333, 1737, 1250, 1514, 1310, 1357, 1721, 1261, 1310, 1353, 1733, 1271, 1310, 1350, 1728, 1271, 1520, 1308, 1340, 1734, 1250, 1521, 1307, 1337, 1738, 1260, 1532, 1309, 1344, 1723, 1281, 1533, 1309, 1347, 1728, 1281, 1309, 1353, 1721, 1281, 1539, 1308, 1333, 1737, 1250, 1550, 1308, 1340, 1733, 1250, 1308, 1320, 1701, 1250]\n",
      "<Instrument_drums>\n",
      "<Note_ON_51>\n",
      "<Velocity_60>\n"
     ]
    }
   ],
   "source": [
    "def encode(tokens):\n",
    "    return [vocab.get(t, vocab[\"<UNK>\"]) for t in tokens]\n",
    "\n",
    "encoded_sequences = [encode(seq) for seq in token_sequences]\n",
    "\n",
    "# test\n",
    "print(encoded_sequences[0])\n",
    "for idx in [1308, 1340, 1726]:\n",
    "    print(itos.get(idx, \"<UNK>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee62314c",
   "metadata": {},
   "source": [
    "Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e1063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 128  # Number of tokens the model sees at once\n",
    "step_size = 8  # How much to slide the window by\n",
    "\n",
    "X, y = [], []\n",
    "for seq in encoded_sequences:\n",
    "    for i in range(0, len(seq) - sequence_length, step_size):\n",
    "        X.append(seq[i:i+sequence_length])\n",
    "        y.append(seq[i+sequence_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b17f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 2060 SUPER\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e82867d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X, dtype=torch.long)\n",
    "y = torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae53facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "dataset = TensorDataset(X, y)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a1abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, hidden_dim=512, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.fc(out[:, -1])  # Use only last output for prediction\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "607c9485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 11239.9710\n",
      "Epoch 2, Loss: 5090.4671\n",
      "Epoch 3, Loss: 4196.6616\n",
      "Epoch 4, Loss: 3783.8724\n",
      "Epoch 5, Loss: 3523.0093\n",
      "Epoch 6, Loss: 3321.9635\n",
      "Epoch 7, Loss: 3171.2735\n",
      "Epoch 8, Loss: 3033.8396\n",
      "Epoch 9, Loss: 2935.0199\n",
      "Epoch 10, Loss: 2849.3381\n"
     ]
    }
   ],
   "source": [
    "# After defining your model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MusicLSTM(vocab_size=len(stoi)).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)  # Move batch to GPU\n",
    "        yb = yb.to(device)  # Move batch to GPU\n",
    "        optimizer.zero_grad()\n",
    "        preds, _ = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e41f96ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_path = project_root / \"music_lstm_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d61861f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rentmtl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
